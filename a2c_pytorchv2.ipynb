{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c-pytorchv2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2Hk7zJ94aSVGoHDel90Iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravjain14/rl-implementations-pytorch/blob/master/a2c_pytorchv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNDCKhEg3D5V",
        "colab_type": "code",
        "outputId": "0e735546-c27b-4626-8158-21972ddc794d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.9.0\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.9.0 in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (4.1.2.30)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (0.15.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (1.17.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (0.25.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (3.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (0.14.1)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.9.0) (3.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (1.4.10)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (6.2.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (0.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.9.0) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.9.0) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.9.0) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.10.9->stable-baselines[mpi]==2.9.0) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.9.0) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzXlK6w-3IKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines.common.cmd_util import *\n",
        "from stable_baselines.common.vec_env import VecFrameStack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_fSJQVpoPDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "\n",
        "import gym\n",
        "\n",
        "render = False\n",
        "update_size = 5\n",
        "num_processes = 16\n",
        "n_stack = 4\n",
        "env_id = 'PongNoFrameskip-v4'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj_OSLxnoX6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feed forward network as described in Asynchronous methods for deep reinforcement learning\n",
        "class ActorCriticFF(nn.Module):\n",
        "  def __init__(self,inp_channels,dimh,dimw,actor_dim,critic_dim):\n",
        "    super(ActorCriticFF,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(inp_channels,32,kernel_size=8,stride=4)\n",
        "    self.conv2 = nn.Conv2d(32,32,kernel_size=4,stride=2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    def conv2d_size_out(size,kernel_size,stride):\n",
        "      return (size - (kernel_size - 1) - 1) // stride + 1    \n",
        "\n",
        "    convw = conv2d_size_out(conv2d_size_out(dimw,8,4),4,2)\n",
        "    convh = conv2d_size_out(conv2d_size_out(dimh,8,4),4,2)\n",
        "\n",
        "    self.linear1 = nn.Linear(convh*convw*32, 256)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.actor = nn.Linear(256,actor_dim)\n",
        "    self.critic = nn.Linear(256,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.relu(self.conv1(x))\n",
        "    x = self.relu(self.conv2(x))\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.relu(self.linear1(x))\n",
        "\n",
        "    actor_out = self.softmax(self.actor(x))\n",
        "    critic_out = self.critic(x)\n",
        "    return actor_out,critic_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Tmw_d7xM7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Is any kind of preprocessing required?\n",
        "def preprocess_pong(x):\n",
        "  resize = transforms.Compose([\n",
        "\t\ttransforms.ToPILImage(), # because pytorch tutorial does so\n",
        "\t\ttransforms.CenterCrop(80),\n",
        "\t\ttransforms.ToTensor()])\n",
        "  \n",
        "  return resize(x).unsqueeze(0)\n",
        "\n",
        "\"\"\" Gradient averaging. \"\"\"\n",
        "def average_gradients(model):\n",
        "    size = float(dist.get_world_size())\n",
        "    for param in model.parameters():\n",
        "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
        "        param.grad.data /= size\n",
        "\n",
        "def discount_rewards_per_proc(rewards,dones,last_value,gamma=0.99):\n",
        "  discounted_r = np.zeros_like(rewards) # should be 1-D\n",
        "  running_add = 0.\n",
        "  if dones[-1] == 0:\n",
        "    running_add = last_value\n",
        "  for t in reversed(range(0,len(rewards))):\n",
        "    running_add = rewards[t] + (1-dones[t])*gamma*running_add\n",
        "    discounted_r[t] = running_add\n",
        "\n",
        "  return discounted_r\n",
        "\n",
        "def discount_rewards_batch(rewards,dones,last_values,gamma=0.99):\n",
        "  # assume rewards shape - num_proc x update_length\n",
        "  batch_size = rewards.shape[0]\n",
        "  discounted_r = np.zeros_like(rewards)\n",
        "  for i in range(batch_size):\n",
        "    discounted_r[i] = discount_rewards_per_proc(rewards[i,:],dones[i,:], \\\n",
        "                                                last_values[i])\n",
        "  return discounted_r\n",
        "\n",
        "def discount_rewards(rewards,dones,last_value,gamma=0.99):\n",
        "  discounted_r = np.zeros_like(rewards)\n",
        "  #running_add = (1-dones[-1,:])*last_value\n",
        "  running_add = np.zeros((1,rewards.shape[1]))\n",
        "  for t in reversed(range(0,update_size)):\n",
        "    running_add = rewards[t,:] + (1-dones[t,:])*(gamma*running_add)\n",
        "    discounted_r[t] = running_add\n",
        "\n",
        "  return discounted_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZxKwiuml3Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Storage():\n",
        "  def __init__(self,obs_dim,num_processes=1,max_depth=5,dtype=np.float32):\n",
        "    self.obs_buf = np.ndarray((max_depth+1,num_processes,*obs_dim),dtype=dtype)\n",
        "    self.rew_buf = np.ndarray((max_depth,num_processes,),dtype=dtype)\n",
        "    self.logp_buf = np.ndarray((max_depth,num_processes,),dtype=dtype)\n",
        "    self.values_buf = np.ndarray((max_depth,num_processes,),dtype=dtype)\n",
        "    self.done_buf = np.ndarray((max_depth,num_processes,),dtype=np.int)\n",
        "    self.last_actions = None\n",
        "    self.curr_idx,self.max_depth = 0,max_depth\n",
        "\n",
        "  def store(self,obs,rew,logp,done,value):\n",
        "    assert(self.curr_idx < self.max_depth)\n",
        "    self.obs_buf[self.curr_idx+1] = obs\n",
        "    self.rew_buf[self.curr_idx] = rew\n",
        "    self.logp_buf[self.curr_idx] = logp\n",
        "    self.done_buf[self.curr_idx] = done\n",
        "    self.values_buf[self.curr_idx] = value\n",
        "    self.curr_idx += 1\n",
        "\n",
        "  def store_last_actions(self,last_actions):\n",
        "    self.last_actions = last_actions\n",
        "\n",
        "  def get(self):\n",
        "    # create tensors\n",
        "    data = {}\n",
        "    data['obs'] = self.obs_buf[1:,].swapaxes(0,1)\n",
        "    data['rew'] = self.rew_buf.swapaxes(0,1)\n",
        "    data['logp'] = self.logp_buf.swapaxes(0,1)\n",
        "    data['done'] = self.done_buf.swapaxes(0,1)\n",
        "    data['value'] = self.values_buf.swapaxes(0,1)\n",
        "\n",
        "    return {k: torch.tensor(v) for k,v in data.items()}\n",
        "\n",
        "  def get_last_actions(self):\n",
        "    return self.last_actions\n",
        "\n",
        "  # store only obs\n",
        "  def store_obs(self,obs,pos=0):\n",
        "    self.obs_buf[pos] = obs\n",
        "\n",
        "  def get_obs(self,pos=0):\n",
        "    return torch.tensor(self.obs_buf[pos])\n",
        "\n",
        "  def rollover(self):\n",
        "    self.obs_buf[0,:] = self.obs_buf[-1,:]    \n",
        "    self.curr_idx = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ATXjwDoqAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trajectory(env,model,preprocess_fn,storage,update_size=5):\n",
        "  reward_infos = []\n",
        "\n",
        "  x = storage.get_obs(0)\n",
        "  for i in range(update_size):\n",
        "    with torch.no_grad():\n",
        "      action_probs,critic_value = model(x)\n",
        "      m = Categorical(action_probs)\n",
        "      action = m.sample()\n",
        "\n",
        "    next_x,rew,done,infos = env.step(action)\n",
        "    for info in infos:\n",
        "      if 'episode' in info.keys():\n",
        "        reward_infos.append(info['episode']['r'])\n",
        "\n",
        "    storage.store(preprocess_fn(next_x), rew, -m.log_prob(action), \\\n",
        "                  done, critic_value.squeeze())\n",
        "    x = storage.get_obs(i)\n",
        "\n",
        "  # store last actions\n",
        "  storage.store_last_actions(action)    \n",
        "  return reward_infos\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo33QuOvolQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update(vec_env,obs,rews,dones,model,optimizer,last_actions, \\\n",
        "            preprocess_fn,value_coeff=0.5,beta=0.01):\n",
        "  action_probs,values = model(obs)\n",
        "  values = values.squeeze()\n",
        "  m = Categorical(action_probs)\n",
        "  actions = m.sample()\n",
        "  logProbs = -m.log_prob(actions)\n",
        "  entropy = m.entropy()\n",
        "\n",
        "  # what my understanding from A3C/A2C is \n",
        "  # perform an action to obtain the next value - for non terminal state\n",
        "  last_obs,last_rew,_,_ = vec_env.step(last_actions)\n",
        "  last_obs = torch.tensor(preprocess_fn(last_obs),dtype=torch.float32)\n",
        "  with torch.no_grad():\n",
        "    _,last_value = model(last_obs)\n",
        "\n",
        "  #new_values = torch.cat((values,torch.zeros(1))).detach()\n",
        "  last_value = last_value.numpy()\n",
        "  dones = dones.numpy()\n",
        "  returns = torch.as_tensor(discount_rewards_batch(rews.numpy(),dones, \\\n",
        "                                  last_value))\n",
        "  returns = returns.view(-1).detach()\n",
        "  advantage = returns - values\n",
        "\n",
        "  # we need to detach advantage, right?\n",
        "  actor_loss = torch.mean(-logProbs*advantage.detach())\n",
        "  mseLoss = nn.MSELoss()\n",
        "  critic_loss = mseLoss(returns,values)\n",
        "\n",
        "  total_loss = actor_loss + value_coeff*critic_loss #- beta*entropy.mean() \n",
        "  total_loss.backward()\n",
        "\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Han3GDYU-T12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_reshape(inp):\n",
        "  # expects numpy array\n",
        "  return inp.transpose((0,3,1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZJRDiFv-i3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rank,preprocess_fn, num_epochs):\n",
        "  # vec_env resets after done. So we need to do this only once\n",
        "  # x is now - num_processes, height, width, num_channels\n",
        "  env = make_atari_env(env_id=env_id,num_env=num_processes,seed=23456)\n",
        "  vec_env = VecFrameStack(env,n_stack=n_stack)  \n",
        "  x = preprocess_fn(vec_env.reset())\n",
        "  n_acts = vec_env.action_space.n\n",
        "\n",
        "  storage = Storage(x.shape[1:],num_processes)\n",
        "  ac = ActorCriticFF(x.shape[1],x.shape[2],x.shape[3],n_acts,1)\n",
        "  optimizer = optim.Adam(ac.parameters(),lr=1e-3)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  storage.store_obs(x)\n",
        "  running_reward_sum = np.zeros((num_processes,1),dtype=np.float32)\n",
        "  running_mean_reward = np.empty((num_processes,1),dtype=np.float32)\n",
        "  running_mean_values = np.empty((num_processes,1),dtype=np.float32)\n",
        "  running_mean_reward.fill(np.inf)\n",
        "  running_mean_values.fill(np.inf)\n",
        "\n",
        "  reward_infos = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    reward_info = trajectory(vec_env,ac,preprocess_fn,storage)\n",
        "    if len(reward_info) > 0:\n",
        "      print(\"reward after %d epochs %f \"% \\\n",
        "            (epoch,sum(reward_info)/len(reward_info)))\n",
        "\n",
        "    data = storage.get()\n",
        "    obs,rews,logp,dones,values = data['obs'], data['rew'], \\\n",
        "                data['logp'],data['done'],data['value']\n",
        "\n",
        "    obs_shape = obs.shape[2:]\n",
        "    update(vec_env,obs.view(-1,*obs_shape),rews,dones,ac,optimizer, \\\n",
        "           storage.get_last_actions(),preprocess_fn)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    storage.rollover()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l485EAThBFwA",
        "colab_type": "code",
        "outputId": "4ac1e8dd-6555-4ba6-bf94-971a324d2f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(0,preprocess_reshape,100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward after 131 epochs -21.000000 \n",
            "reward after 136 epochs -21.000000 \n",
            "reward after 139 epochs -21.000000 \n",
            "reward after 141 epochs -21.000000 \n",
            "reward after 145 epochs -21.000000 \n",
            "reward after 148 epochs -20.000000 \n",
            "reward after 149 epochs -20.000000 \n",
            "reward after 154 epochs -19.000000 \n",
            "reward after 155 epochs -20.000000 \n",
            "reward after 159 epochs -20.000000 \n",
            "reward after 166 epochs -20.000000 \n",
            "reward after 167 epochs -19.000000 \n",
            "reward after 173 epochs -19.000000 \n",
            "reward after 185 epochs -19.000000 \n",
            "reward after 265 epochs -21.000000 \n",
            "reward after 287 epochs -21.000000 \n",
            "reward after 296 epochs -20.000000 \n",
            "reward after 297 epochs -20.000000 \n",
            "reward after 312 epochs -20.000000 \n",
            "reward after 315 epochs -20.000000 \n",
            "reward after 319 epochs -19.000000 \n",
            "reward after 322 epochs -19.000000 \n",
            "reward after 326 epochs -21.000000 \n",
            "reward after 328 epochs -20.000000 \n",
            "reward after 330 epochs -19.000000 \n",
            "reward after 391 epochs -21.000000 \n",
            "reward after 439 epochs -20.000000 \n",
            "reward after 445 epochs -20.000000 \n",
            "reward after 449 epochs -21.000000 \n",
            "reward after 456 epochs -21.000000 \n",
            "reward after 461 epochs -20.500000 \n",
            "reward after 465 epochs -20.000000 \n",
            "reward after 467 epochs -21.000000 \n",
            "reward after 471 epochs -20.000000 \n",
            "reward after 478 epochs -21.000000 \n",
            "reward after 492 epochs -20.000000 \n",
            "reward after 507 epochs -21.000000 \n",
            "reward after 518 epochs -21.000000 \n",
            "reward after 565 epochs -21.000000 \n",
            "reward after 582 epochs -21.000000 \n",
            "reward after 587 epochs -21.000000 \n",
            "reward after 591 epochs -21.000000 \n",
            "reward after 594 epochs -20.000000 \n",
            "reward after 603 epochs -21.000000 \n",
            "reward after 606 epochs -20.000000 \n",
            "reward after 610 epochs -21.000000 \n",
            "reward after 623 epochs -21.000000 \n",
            "reward after 628 epochs -21.000000 \n",
            "reward after 655 epochs -21.000000 \n",
            "reward after 661 epochs -20.000000 \n",
            "reward after 701 epochs -21.000000 \n",
            "reward after 717 epochs -21.000000 \n",
            "reward after 720 epochs -21.000000 \n",
            "reward after 732 epochs -19.000000 \n",
            "reward after 737 epochs -21.000000 \n",
            "reward after 743 epochs -21.000000 \n",
            "reward after 746 epochs -21.000000 \n",
            "reward after 748 epochs -21.000000 \n",
            "reward after 752 epochs -20.000000 \n",
            "reward after 775 epochs -21.000000 \n",
            "reward after 781 epochs -21.000000 \n",
            "reward after 787 epochs -21.000000 \n",
            "reward after 856 epochs -21.000000 \n",
            "reward after 859 epochs -21.000000 \n",
            "reward after 867 epochs -21.000000 \n",
            "reward after 869 epochs -21.000000 \n",
            "reward after 870 epochs -19.000000 \n",
            "reward after 896 epochs -20.000000 \n",
            "reward after 914 epochs -20.000000 \n",
            "reward after 940 epochs -20.000000 \n",
            "reward after 969 epochs -19.000000 \n",
            "reward after 995 epochs -21.000000 \n",
            "reward after 1001 epochs -21.000000 \n",
            "reward after 1010 epochs -21.000000 \n",
            "reward after 1023 epochs -21.000000 \n",
            "reward after 1030 epochs -20.000000 \n",
            "reward after 1033 epochs -20.000000 \n",
            "reward after 1034 epochs -19.000000 \n",
            "reward after 1037 epochs -20.000000 \n",
            "reward after 1038 epochs -21.000000 \n",
            "reward after 1043 epochs -19.000000 \n",
            "reward after 1071 epochs -19.000000 \n",
            "reward after 1099 epochs -20.000000 \n",
            "reward after 1122 epochs -20.000000 \n",
            "reward after 1125 epochs -21.000000 \n",
            "reward after 1167 epochs -20.000000 \n",
            "reward after 1168 epochs -21.000000 \n",
            "reward after 1178 epochs -20.000000 \n",
            "reward after 1185 epochs -21.000000 \n",
            "reward after 1188 epochs -19.000000 \n",
            "reward after 1192 epochs -20.000000 \n",
            "reward after 1206 epochs -20.000000 \n",
            "reward after 1219 epochs -20.000000 \n",
            "reward after 1231 epochs -16.000000 \n",
            "reward after 1252 epochs -21.000000 \n",
            "reward after 1265 epochs -20.000000 \n",
            "reward after 1284 epochs -21.000000 \n",
            "reward after 1297 epochs -21.000000 \n",
            "reward after 1311 epochs -17.000000 \n",
            "reward after 1334 epochs -20.000000 \n",
            "reward after 1335 epochs -21.000000 \n",
            "reward after 1345 epochs -21.000000 \n",
            "reward after 1350 epochs -20.000000 \n",
            "reward after 1356 epochs -20.000000 \n",
            "reward after 1369 epochs -19.000000 \n",
            "reward after 1379 epochs -21.000000 \n",
            "reward after 1396 epochs -19.000000 \n",
            "reward after 1405 epochs -20.000000 \n",
            "reward after 1409 epochs -20.000000 \n",
            "reward after 1449 epochs -20.000000 \n",
            "reward after 1465 epochs -21.000000 \n",
            "reward after 1469 epochs -21.000000 \n",
            "reward after 1471 epochs -21.000000 \n",
            "reward after 1482 epochs -21.000000 \n",
            "reward after 1488 epochs -19.000000 \n",
            "reward after 1505 epochs -16.000000 \n",
            "reward after 1508 epochs -21.000000 \n",
            "reward after 1516 epochs -19.000000 \n",
            "reward after 1522 epochs -20.000000 \n",
            "reward after 1533 epochs -21.000000 \n",
            "reward after 1548 epochs -20.000000 \n",
            "reward after 1553 epochs -20.000000 \n",
            "reward after 1561 epochs -21.000000 \n",
            "reward after 1566 epochs -19.000000 \n",
            "reward after 1576 epochs -21.000000 \n",
            "reward after 1591 epochs -21.000000 \n",
            "reward after 1600 epochs -21.000000 \n",
            "reward after 1608 epochs -21.000000 \n",
            "reward after 1617 epochs -21.000000 \n",
            "reward after 1634 epochs -21.000000 \n",
            "reward after 1643 epochs -21.000000 \n",
            "reward after 1660 epochs -20.000000 \n",
            "reward after 1663 epochs -21.000000 \n",
            "reward after 1675 epochs -20.000000 \n",
            "reward after 1679 epochs -21.000000 \n",
            "reward after 1692 epochs -21.000000 \n",
            "reward after 1700 epochs -21.000000 \n",
            "reward after 1701 epochs -20.000000 \n",
            "reward after 1718 epochs -21.000000 \n",
            "reward after 1719 epochs -21.000000 \n",
            "reward after 1737 epochs -21.000000 \n",
            "reward after 1756 epochs -20.000000 \n",
            "reward after 1765 epochs -21.000000 \n",
            "reward after 1780 epochs -21.000000 \n",
            "reward after 1782 epochs -20.000000 \n",
            "reward after 1802 epochs -20.000000 \n",
            "reward after 1823 epochs -21.000000 \n",
            "reward after 1825 epochs -20.000000 \n",
            "reward after 1826 epochs -21.000000 \n",
            "reward after 1829 epochs -20.000000 \n",
            "reward after 1836 epochs -21.000000 \n",
            "reward after 1839 epochs -20.000000 \n",
            "reward after 1845 epochs -21.000000 \n",
            "reward after 1851 epochs -20.000000 \n",
            "reward after 1858 epochs -21.000000 \n",
            "reward after 1860 epochs -21.000000 \n",
            "reward after 1889 epochs -20.000000 \n",
            "reward after 1911 epochs -21.000000 \n",
            "reward after 1921 epochs -20.000000 \n",
            "reward after 1924 epochs -20.000000 \n",
            "reward after 1949 epochs -21.000000 \n",
            "reward after 1956 epochs -20.000000 \n",
            "reward after 1958 epochs -21.000000 \n",
            "reward after 1972 epochs -21.000000 \n",
            "reward after 1975 epochs -21.000000 \n",
            "reward after 1979 epochs -21.000000 \n",
            "reward after 2000 epochs -21.000000 \n",
            "reward after 2026 epochs -18.000000 \n",
            "reward after 2033 epochs -20.000000 \n",
            "reward after 2065 epochs -21.000000 \n",
            "reward after 2073 epochs -20.000000 \n",
            "reward after 2102 epochs -20.000000 \n",
            "reward after 2110 epochs -19.000000 \n",
            "reward after 2116 epochs -20.000000 \n",
            "reward after 2120 epochs -21.000000 \n",
            "reward after 2127 epochs -21.000000 \n",
            "reward after 2130 epochs -19.000000 \n",
            "reward after 2147 epochs -21.000000 \n",
            "reward after 2215 epochs -20.000000 \n",
            "reward after 2235 epochs -21.000000 \n",
            "reward after 2237 epochs -20.000000 \n",
            "reward after 2248 epochs -21.000000 \n",
            "reward after 2249 epochs -20.000000 \n",
            "reward after 2251 epochs -21.000000 \n",
            "reward after 2273 epochs -18.000000 \n",
            "reward after 2280 epochs -20.000000 \n",
            "reward after 2284 epochs -19.000000 \n",
            "reward after 2298 epochs -19.000000 \n",
            "reward after 2331 epochs -21.000000 \n",
            "reward after 2339 epochs -18.000000 \n",
            "reward after 2345 epochs -21.000000 \n",
            "reward after 2346 epochs -21.000000 \n",
            "reward after 2377 epochs -21.000000 \n",
            "reward after 2378 epochs -21.000000 \n",
            "reward after 2381 epochs -21.000000 \n",
            "reward after 2421 epochs -20.000000 \n",
            "reward after 2427 epochs -20.000000 \n",
            "reward after 2433 epochs -20.000000 \n",
            "reward after 2439 epochs -21.000000 \n",
            "reward after 2444 epochs -19.000000 \n",
            "reward after 2448 epochs -17.000000 \n",
            "reward after 2483 epochs -20.000000 \n",
            "reward after 2493 epochs -21.000000 \n",
            "reward after 2502 epochs -19.000000 \n",
            "reward after 2522 epochs -20.000000 \n",
            "reward after 2546 epochs -20.000000 \n",
            "reward after 2564 epochs -20.000000 \n",
            "reward after 2588 epochs -21.000000 \n",
            "reward after 2595 epochs -20.000000 \n",
            "reward after 2604 epochs -20.000000 \n",
            "reward after 2613 epochs -20.000000 \n",
            "reward after 2621 epochs -20.000000 \n",
            "reward after 2632 epochs -21.000000 \n",
            "reward after 2644 epochs -17.000000 \n",
            "reward after 2650 epochs -21.000000 \n",
            "reward after 2662 epochs -21.000000 \n",
            "reward after 2676 epochs -21.000000 \n",
            "reward after 2685 epochs -19.000000 \n",
            "reward after 2703 epochs -20.000000 \n",
            "reward after 2715 epochs -20.000000 \n",
            "reward after 2735 epochs -21.000000 \n",
            "reward after 2737 epochs -21.000000 \n",
            "reward after 2744 epochs -20.000000 \n",
            "reward after 2753 epochs -20.000000 \n",
            "reward after 2758 epochs -19.000000 \n",
            "reward after 2787 epochs -20.000000 \n",
            "reward after 2788 epochs -19.000000 \n",
            "reward after 2826 epochs -20.000000 \n",
            "reward after 2827 epochs -19.000000 \n",
            "reward after 2837 epochs -19.000000 \n",
            "reward after 2838 epochs -17.000000 \n",
            "reward after 2855 epochs -19.000000 \n",
            "reward after 2862 epochs -21.000000 \n",
            "reward after 2865 epochs -21.000000 \n",
            "reward after 2902 epochs -20.000000 \n",
            "reward after 2921 epochs -18.000000 \n",
            "reward after 2926 epochs -20.000000 \n",
            "reward after 2931 epochs -19.000000 \n",
            "reward after 2932 epochs -21.000000 \n",
            "reward after 2935 epochs -20.000000 \n",
            "reward after 2956 epochs -21.000000 \n",
            "reward after 2965 epochs -19.000000 \n",
            "reward after 3001 epochs -20.000000 \n",
            "reward after 3023 epochs -20.000000 \n",
            "reward after 3025 epochs -21.000000 \n",
            "reward after 3031 epochs -19.000000 \n",
            "reward after 3033 epochs -21.000000 \n",
            "reward after 3053 epochs -19.000000 \n",
            "reward after 3074 epochs -20.000000 \n",
            "reward after 3078 epochs -21.000000 \n",
            "reward after 3094 epochs -19.000000 \n",
            "reward after 3107 epochs -21.000000 \n",
            "reward after 3116 epochs -21.000000 \n",
            "reward after 3123 epochs -19.000000 \n",
            "reward after 3132 epochs -20.000000 \n",
            "reward after 3161 epochs -21.000000 \n",
            "reward after 3167 epochs -21.000000 \n",
            "reward after 3170 epochs -20.500000 \n",
            "reward after 3178 epochs -21.000000 \n",
            "reward after 3190 epochs -19.000000 \n",
            "reward after 3208 epochs -20.000000 \n",
            "reward after 3241 epochs -21.000000 \n",
            "reward after 3242 epochs -20.000000 \n",
            "reward after 3267 epochs -20.000000 \n",
            "reward after 3274 epochs -19.500000 \n",
            "reward after 3282 epochs -20.000000 \n",
            "reward after 3306 epochs -20.000000 \n",
            "reward after 3314 epochs -20.000000 \n",
            "reward after 3318 epochs -19.000000 \n",
            "reward after 3320 epochs -20.000000 \n",
            "reward after 3321 epochs -19.000000 \n",
            "reward after 3340 epochs -20.000000 \n",
            "reward after 3371 epochs -21.000000 \n",
            "reward after 3372 epochs -21.000000 \n",
            "reward after 3373 epochs -20.000000 \n",
            "reward after 3410 epochs -21.000000 \n",
            "reward after 3419 epochs -21.000000 \n",
            "reward after 3424 epochs -20.000000 \n",
            "reward after 3452 epochs -21.000000 \n",
            "reward after 3464 epochs -20.000000 \n",
            "reward after 3470 epochs -20.000000 \n",
            "reward after 3471 epochs -21.000000 \n",
            "reward after 3484 epochs -20.000000 \n",
            "reward after 3487 epochs -20.000000 \n",
            "reward after 3496 epochs -20.000000 \n",
            "reward after 3498 epochs -20.000000 \n",
            "reward after 3503 epochs -21.000000 \n",
            "reward after 3504 epochs -21.000000 \n",
            "reward after 3525 epochs -21.000000 \n",
            "reward after 3548 epochs -21.000000 \n",
            "reward after 3566 epochs -21.000000 \n",
            "reward after 3586 epochs -21.000000 \n",
            "reward after 3604 epochs -20.000000 \n",
            "reward after 3611 epochs -21.000000 \n",
            "reward after 3612 epochs -21.000000 \n",
            "reward after 3616 epochs -21.000000 \n",
            "reward after 3624 epochs -21.000000 \n",
            "reward after 3630 epochs -21.000000 \n",
            "reward after 3636 epochs -20.000000 \n",
            "reward after 3644 epochs -20.000000 \n",
            "reward after 3653 epochs -20.000000 \n",
            "reward after 3678 epochs -21.000000 \n",
            "reward after 3711 epochs -21.000000 \n",
            "reward after 3729 epochs -20.000000 \n",
            "reward after 3730 epochs -20.000000 \n",
            "reward after 3751 epochs -20.000000 \n",
            "reward after 3752 epochs -21.000000 \n",
            "reward after 3753 epochs -18.000000 \n",
            "reward after 3754 epochs -20.000000 \n",
            "reward after 3774 epochs -19.000000 \n",
            "reward after 3786 epochs -19.000000 \n",
            "reward after 3789 epochs -20.000000 \n",
            "reward after 3805 epochs -20.000000 \n",
            "reward after 3807 epochs -21.000000 \n",
            "reward after 3826 epochs -20.000000 \n",
            "reward after 3840 epochs -18.000000 \n",
            "reward after 3857 epochs -20.000000 \n",
            "reward after 3872 epochs -21.000000 \n",
            "reward after 3890 epochs -21.000000 \n",
            "reward after 3892 epochs -20.000000 \n",
            "reward after 3901 epochs -20.000000 \n",
            "reward after 3902 epochs -21.000000 \n",
            "reward after 3915 epochs -21.000000 \n",
            "reward after 3933 epochs -21.000000 \n",
            "reward after 3935 epochs -21.000000 \n",
            "reward after 3951 epochs -21.000000 \n",
            "reward after 3957 epochs -21.000000 \n",
            "reward after 4009 epochs -21.000000 \n",
            "reward after 4017 epochs -20.000000 \n",
            "reward after 4023 epochs -19.000000 \n",
            "reward after 4030 epochs -21.000000 \n",
            "reward after 4032 epochs -21.000000 \n",
            "reward after 4058 epochs -19.000000 \n",
            "reward after 4063 epochs -21.000000 \n",
            "reward after 4064 epochs -21.000000 \n",
            "reward after 4068 epochs -20.000000 \n",
            "reward after 4083 epochs -21.000000 \n",
            "reward after 4090 epochs -20.000000 \n",
            "reward after 4110 epochs -20.000000 \n",
            "reward after 4149 epochs -21.000000 \n",
            "reward after 4157 epochs -20.000000 \n",
            "reward after 4159 epochs -21.000000 \n",
            "reward after 4161 epochs -21.000000 \n",
            "reward after 4164 epochs -21.000000 \n",
            "reward after 4193 epochs -20.000000 \n",
            "reward after 4204 epochs -21.000000 \n",
            "reward after 4206 epochs -21.000000 \n",
            "reward after 4217 epochs -20.000000 \n",
            "reward after 4218 epochs -21.000000 \n",
            "reward after 4219 epochs -21.000000 \n",
            "reward after 4222 epochs -20.000000 \n",
            "reward after 4296 epochs -20.000000 \n",
            "reward after 4308 epochs -21.000000 \n",
            "reward after 4314 epochs -20.000000 \n",
            "reward after 4320 epochs -20.000000 \n",
            "reward after 4352 epochs -21.000000 \n",
            "reward after 4361 epochs -20.000000 \n",
            "reward after 4368 epochs -21.000000 \n",
            "reward after 4372 epochs -20.000000 \n",
            "reward after 4374 epochs -20.000000 \n",
            "reward after 4387 epochs -19.500000 \n",
            "reward after 4421 epochs -21.000000 \n",
            "reward after 4435 epochs -20.000000 \n",
            "reward after 4439 epochs -21.000000 \n",
            "reward after 4443 epochs -21.000000 \n",
            "reward after 4446 epochs -20.000000 \n",
            "reward after 4480 epochs -21.000000 \n",
            "reward after 4490 epochs -21.000000 \n",
            "reward after 4491 epochs -20.000000 \n",
            "reward after 4497 epochs -21.000000 \n",
            "reward after 4503 epochs -20.500000 \n",
            "reward after 4510 epochs -20.000000 \n",
            "reward after 4517 epochs -21.000000 \n",
            "reward after 4523 epochs -21.000000 \n",
            "reward after 4531 epochs -20.000000 \n",
            "reward after 4564 epochs -20.000000 \n",
            "reward after 4580 epochs -21.000000 \n",
            "reward after 4588 epochs -20.000000 \n",
            "reward after 4618 epochs -20.000000 \n",
            "reward after 4621 epochs -20.000000 \n",
            "reward after 4628 epochs -21.000000 \n",
            "reward after 4629 epochs -20.000000 \n",
            "reward after 4637 epochs -20.000000 \n",
            "reward after 4639 epochs -21.000000 \n",
            "reward after 4650 epochs -20.500000 \n",
            "reward after 4657 epochs -20.000000 \n",
            "reward after 4659 epochs -21.000000 \n",
            "reward after 4661 epochs -21.000000 \n",
            "reward after 4701 epochs -21.000000 \n",
            "reward after 4703 epochs -20.000000 \n",
            "reward after 4726 epochs -20.000000 \n",
            "reward after 4733 epochs -20.000000 \n",
            "reward after 4760 epochs -20.000000 \n",
            "reward after 4769 epochs -20.000000 \n",
            "reward after 4778 epochs -20.000000 \n",
            "reward after 4780 epochs -21.000000 \n",
            "reward after 4795 epochs -21.000000 \n",
            "reward after 4796 epochs -21.000000 \n",
            "reward after 4800 epochs -21.000000 \n",
            "reward after 4806 epochs -21.000000 \n",
            "reward after 4840 epochs -21.000000 \n",
            "reward after 4850 epochs -20.000000 \n",
            "reward after 4856 epochs -21.000000 \n",
            "reward after 4862 epochs -21.000000 \n",
            "reward after 4865 epochs -20.000000 \n",
            "reward after 4916 epochs -20.000000 \n",
            "reward after 4925 epochs -21.000000 \n",
            "reward after 4929 epochs -20.000000 \n",
            "reward after 4933 epochs -21.000000 \n",
            "reward after 4936 epochs -21.000000 \n",
            "reward after 4940 epochs -20.000000 \n",
            "reward after 4970 epochs -21.000000 \n",
            "reward after 4986 epochs -21.000000 \n",
            "reward after 4997 epochs -21.000000 \n",
            "reward after 5001 epochs -21.000000 \n",
            "reward after 5004 epochs -20.000000 \n",
            "reward after 5028 epochs -21.000000 \n",
            "reward after 5029 epochs -20.000000 \n",
            "reward after 5044 epochs -20.000000 \n",
            "reward after 5061 epochs -21.000000 \n",
            "reward after 5066 epochs -21.000000 \n",
            "reward after 5075 epochs -20.000000 \n",
            "reward after 5077 epochs -21.000000 \n",
            "reward after 5085 epochs -21.000000 \n",
            "reward after 5103 epochs -20.000000 \n",
            "reward after 5116 epochs -21.000000 \n",
            "reward after 5120 epochs -21.000000 \n",
            "reward after 5138 epochs -21.000000 \n",
            "reward after 5143 epochs -20.000000 \n",
            "reward after 5164 epochs -21.000000 \n",
            "reward after 5180 epochs -19.000000 \n",
            "reward after 5183 epochs -20.000000 \n",
            "reward after 5215 epochs -20.000000 \n",
            "reward after 5216 epochs -20.000000 \n",
            "reward after 5221 epochs -20.000000 \n",
            "reward after 5245 epochs -21.000000 \n",
            "reward after 5247 epochs -20.000000 \n",
            "reward after 5259 epochs -21.000000 \n",
            "reward after 5273 epochs -21.000000 \n",
            "reward after 5275 epochs -21.000000 \n",
            "reward after 5303 epochs -20.000000 \n",
            "reward after 5319 epochs -21.000000 \n",
            "reward after 5335 epochs -20.000000 \n",
            "reward after 5342 epochs -20.000000 \n",
            "reward after 5355 epochs -20.000000 \n",
            "reward after 5359 epochs -20.000000 \n",
            "reward after 5399 epochs -20.000000 \n",
            "reward after 5413 epochs -20.000000 \n",
            "reward after 5437 epochs -19.000000 \n",
            "reward after 5442 epochs -21.000000 \n",
            "reward after 5465 epochs -21.000000 \n",
            "reward after 5474 epochs -21.000000 \n",
            "reward after 5483 epochs -20.000000 \n",
            "reward after 5490 epochs -21.000000 \n",
            "reward after 5497 epochs -19.000000 \n",
            "reward after 5500 epochs -20.000000 \n",
            "reward after 5502 epochs -21.000000 \n",
            "reward after 5527 epochs -20.000000 \n",
            "reward after 5528 epochs -20.000000 \n",
            "reward after 5536 epochs -21.000000 \n",
            "reward after 5543 epochs -21.000000 \n",
            "reward after 5556 epochs -19.000000 \n",
            "reward after 5567 epochs -19.000000 \n",
            "reward after 5600 epochs -19.000000 \n",
            "reward after 5613 epochs -20.000000 \n",
            "reward after 5623 epochs -20.000000 \n",
            "reward after 5638 epochs -21.000000 \n",
            "reward after 5657 epochs -21.000000 \n",
            "reward after 5665 epochs -21.000000 \n",
            "reward after 5675 epochs -20.000000 \n",
            "reward after 5690 epochs -21.000000 \n",
            "reward after 5696 epochs -21.000000 \n",
            "reward after 5750 epochs -20.500000 \n",
            "reward after 5752 epochs -20.000000 \n",
            "reward after 5767 epochs -21.000000 \n",
            "reward after 5777 epochs -20.000000 \n",
            "reward after 5796 epochs -20.000000 \n",
            "reward after 5815 epochs -20.000000 \n",
            "reward after 5816 epochs -20.000000 \n",
            "reward after 5826 epochs -21.000000 \n",
            "reward after 5828 epochs -20.000000 \n",
            "reward after 5872 epochs -21.000000 \n",
            "reward after 5880 epochs -21.000000 \n",
            "reward after 5888 epochs -20.000000 \n",
            "reward after 5889 epochs -21.000000 \n",
            "reward after 5896 epochs -21.000000 \n",
            "reward after 5902 epochs -20.000000 \n",
            "reward after 5916 epochs -20.000000 \n",
            "reward after 5926 epochs -20.000000 \n",
            "reward after 5928 epochs -21.000000 \n",
            "reward after 5931 epochs -21.000000 \n",
            "reward after 5949 epochs -21.000000 \n",
            "reward after 5960 epochs -21.000000 \n",
            "reward after 5975 epochs -20.500000 \n",
            "reward after 6003 epochs -21.000000 \n",
            "reward after 6027 epochs -20.000000 \n",
            "reward after 6029 epochs -20.000000 \n",
            "reward after 6036 epochs -20.000000 \n",
            "reward after 6042 epochs -20.000000 \n",
            "reward after 6048 epochs -20.000000 \n",
            "reward after 6065 epochs -21.000000 \n",
            "reward after 6067 epochs -20.000000 \n",
            "reward after 6070 epochs -20.000000 \n",
            "reward after 6076 epochs -20.000000 \n",
            "reward after 6099 epochs -20.000000 \n",
            "reward after 6105 epochs -21.000000 \n",
            "reward after 6115 epochs -21.000000 \n",
            "reward after 6137 epochs -19.000000 \n",
            "reward after 6162 epochs -20.500000 \n",
            "reward after 6163 epochs -21.000000 \n",
            "reward after 6172 epochs -21.000000 \n",
            "reward after 6176 epochs -20.000000 \n",
            "reward after 6178 epochs -21.000000 \n",
            "reward after 6198 epochs -20.000000 \n",
            "reward after 6209 epochs -20.000000 \n",
            "reward after 6212 epochs -21.000000 \n",
            "reward after 6219 epochs -21.000000 \n",
            "reward after 6224 epochs -20.000000 \n",
            "reward after 6231 epochs -19.000000 \n",
            "reward after 6242 epochs -21.000000 \n",
            "reward after 6249 epochs -20.000000 \n",
            "reward after 6262 epochs -21.000000 \n",
            "reward after 6291 epochs -21.000000 \n",
            "reward after 6293 epochs -21.000000 \n",
            "reward after 6305 epochs -20.000000 \n",
            "reward after 6308 epochs -21.000000 \n",
            "reward after 6311 epochs -20.000000 \n",
            "reward after 6338 epochs -20.000000 \n",
            "reward after 6359 epochs -20.500000 \n",
            "reward after 6360 epochs -21.000000 \n",
            "reward after 6379 epochs -21.000000 \n",
            "reward after 6388 epochs -21.000000 \n",
            "reward after 6391 epochs -21.000000 \n",
            "reward after 6399 epochs -21.000000 \n",
            "reward after 6415 epochs -20.000000 \n",
            "reward after 6429 epochs -20.000000 \n",
            "reward after 6431 epochs -20.000000 \n",
            "reward after 6441 epochs -21.000000 \n",
            "reward after 6446 epochs -20.000000 \n",
            "reward after 6449 epochs -21.000000 \n",
            "reward after 6469 epochs -20.000000 \n",
            "reward after 6476 epochs -20.000000 \n",
            "reward after 6488 epochs -21.000000 \n",
            "reward after 6497 epochs -21.000000 \n",
            "reward after 6516 epochs -21.000000 \n",
            "reward after 6518 epochs -20.000000 \n",
            "reward after 6527 epochs -20.000000 \n",
            "reward after 6530 epochs -20.000000 \n",
            "reward after 6537 epochs -20.000000 \n",
            "reward after 6546 epochs -21.000000 \n",
            "reward after 6559 epochs -21.000000 \n",
            "reward after 6571 epochs -20.000000 \n",
            "reward after 6580 epochs -20.000000 \n",
            "reward after 6582 epochs -21.000000 \n",
            "reward after 6608 epochs -20.000000 \n",
            "reward after 6610 epochs -21.000000 \n",
            "reward after 6623 epochs -21.000000 \n",
            "reward after 6624 epochs -21.000000 \n",
            "reward after 6625 epochs -21.000000 \n",
            "reward after 6654 epochs -21.000000 \n",
            "reward after 6655 epochs -20.000000 \n",
            "reward after 6657 epochs -21.000000 \n",
            "reward after 6666 epochs -21.000000 \n",
            "reward after 6676 epochs -20.000000 \n",
            "reward after 6716 epochs -21.000000 \n",
            "reward after 6719 epochs -21.000000 \n",
            "reward after 6726 epochs -21.000000 \n",
            "reward after 6738 epochs -20.000000 \n",
            "reward after 6740 epochs -21.000000 \n",
            "reward after 6753 epochs -21.000000 \n",
            "reward after 6764 epochs -20.000000 \n",
            "reward after 6773 epochs -20.000000 \n",
            "reward after 6796 epochs -20.000000 \n",
            "reward after 6816 epochs -20.000000 \n",
            "reward after 6817 epochs -21.000000 \n",
            "reward after 6856 epochs -21.000000 \n",
            "reward after 6857 epochs -21.000000 \n",
            "reward after 6887 epochs -20.000000 \n",
            "reward after 6900 epochs -21.000000 \n",
            "reward after 6903 epochs -20.000000 \n",
            "reward after 6914 epochs -20.000000 \n",
            "reward after 6923 epochs -21.000000 \n",
            "reward after 6924 epochs -21.000000 \n",
            "reward after 6948 epochs -20.000000 \n",
            "reward after 6955 epochs -20.000000 \n",
            "reward after 6983 epochs -20.000000 \n",
            "reward after 6993 epochs -21.000000 \n",
            "reward after 6996 epochs -21.000000 \n",
            "reward after 7005 epochs -20.000000 \n",
            "reward after 7017 epochs -20.000000 \n",
            "reward after 7022 epochs -21.000000 \n",
            "reward after 7039 epochs -21.000000 \n",
            "reward after 7043 epochs -21.000000 \n",
            "reward after 7049 epochs -20.000000 \n",
            "reward after 7053 epochs -20.000000 \n",
            "reward after 7054 epochs -21.000000 \n",
            "reward after 7092 epochs -21.000000 \n",
            "reward after 7094 epochs -20.000000 \n",
            "reward after 7097 epochs -20.000000 \n",
            "reward after 7123 epochs -20.000000 \n",
            "reward after 7124 epochs -21.000000 \n",
            "reward after 7125 epochs -21.000000 \n",
            "reward after 7134 epochs -21.000000 \n",
            "reward after 7147 epochs -20.000000 \n",
            "reward after 7161 epochs -20.000000 \n",
            "reward after 7179 epochs -21.000000 \n",
            "reward after 7183 epochs -20.000000 \n",
            "reward after 7188 epochs -20.000000 \n",
            "reward after 7193 epochs -20.000000 \n",
            "reward after 7218 epochs -20.000000 \n",
            "reward after 7222 epochs -21.000000 \n",
            "reward after 7231 epochs -21.000000 \n",
            "reward after 7257 epochs -20.000000 \n",
            "reward after 7262 epochs -20.000000 \n",
            "reward after 7264 epochs -21.000000 \n",
            "reward after 7265 epochs -20.000000 \n",
            "reward after 7277 epochs -21.000000 \n",
            "reward after 7280 epochs -19.000000 \n",
            "reward after 7300 epochs -20.000000 \n",
            "reward after 7307 epochs -21.000000 \n",
            "reward after 7313 epochs -21.000000 \n",
            "reward after 7318 epochs -21.000000 \n",
            "reward after 7347 epochs -21.000000 \n",
            "reward after 7362 epochs -21.000000 \n",
            "reward after 7373 epochs -20.000000 \n",
            "reward after 7378 epochs -21.000000 \n",
            "reward after 7395 epochs -21.000000 \n",
            "reward after 7398 epochs -21.000000 \n",
            "reward after 7402 epochs -20.000000 \n",
            "reward after 7419 epochs -20.000000 \n",
            "reward after 7422 epochs -20.000000 \n",
            "reward after 7439 epochs -20.000000 \n",
            "reward after 7446 epochs -20.000000 \n",
            "reward after 7457 epochs -20.000000 \n",
            "reward after 7467 epochs -20.000000 \n",
            "reward after 7483 epochs -21.000000 \n",
            "reward after 7491 epochs -21.000000 \n",
            "reward after 7512 epochs -20.000000 \n",
            "reward after 7519 epochs -21.000000 \n",
            "reward after 7534 epochs -20.000000 \n",
            "reward after 7537 epochs -20.000000 \n",
            "reward after 7541 epochs -20.000000 \n",
            "reward after 7558 epochs -20.000000 \n",
            "reward after 7568 epochs -20.000000 \n",
            "reward after 7575 epochs -21.000000 \n",
            "reward after 7585 epochs -20.000000 \n",
            "reward after 7594 epochs -21.000000 \n",
            "reward after 7619 epochs -21.000000 \n",
            "reward after 7623 epochs -20.000000 \n",
            "reward after 7629 epochs -20.000000 \n",
            "reward after 7630 epochs -20.000000 \n",
            "reward after 7641 epochs -21.000000 \n",
            "reward after 7658 epochs -20.000000 \n",
            "reward after 7668 epochs -21.000000 \n",
            "reward after 7694 epochs -21.000000 \n",
            "reward after 7698 epochs -20.000000 \n",
            "reward after 7703 epochs -20.000000 \n",
            "reward after 7705 epochs -21.000000 \n",
            "reward after 7723 epochs -20.000000 \n",
            "reward after 7730 epochs -21.000000 \n",
            "reward after 7758 epochs -20.000000 \n",
            "reward after 7760 epochs -21.000000 \n",
            "reward after 7766 epochs -21.000000 \n",
            "reward after 7794 epochs -21.000000 \n",
            "reward after 7802 epochs -21.000000 \n",
            "reward after 7822 epochs -21.000000 \n",
            "reward after 7837 epochs -20.000000 \n",
            "reward after 7841 epochs -21.000000 \n",
            "reward after 7843 epochs -20.000000 \n",
            "reward after 7853 epochs -20.000000 \n",
            "reward after 7854 epochs -21.000000 \n",
            "reward after 7864 epochs -20.000000 \n",
            "reward after 7875 epochs -20.000000 \n",
            "reward after 7879 epochs -20.000000 \n",
            "reward after 7889 epochs -21.000000 \n",
            "reward after 7900 epochs -20.000000 \n",
            "reward after 7903 epochs -21.000000 \n",
            "reward after 7913 epochs -20.000000 \n",
            "reward after 7937 epochs -20.000000 \n",
            "reward after 7939 epochs -20.000000 \n",
            "reward after 7951 epochs -20.000000 \n",
            "reward after 7968 epochs -21.000000 \n",
            "reward after 7974 epochs -21.000000 \n",
            "reward after 7989 epochs -21.000000 \n",
            "reward after 7990 epochs -20.000000 \n",
            "reward after 7992 epochs -20.000000 \n",
            "reward after 7997 epochs -20.000000 \n",
            "reward after 8000 epochs -21.000000 \n",
            "reward after 8009 epochs -21.000000 \n",
            "reward after 8014 epochs -20.000000 \n",
            "reward after 8025 epochs -21.000000 \n",
            "reward after 8036 epochs -21.000000 \n",
            "reward after 8039 epochs -21.000000 \n",
            "reward after 8053 epochs -20.000000 \n",
            "reward after 8080 epochs -21.000000 \n",
            "reward after 8089 epochs -20.000000 \n",
            "reward after 8107 epochs -20.000000 \n",
            "reward after 8120 epochs -21.000000 \n",
            "reward after 8124 epochs -20.000000 \n",
            "reward after 8128 epochs -20.000000 \n",
            "reward after 8130 epochs -21.000000 \n",
            "reward after 8147 epochs -20.000000 \n",
            "reward after 8156 epochs -21.000000 \n",
            "reward after 8180 epochs -20.000000 \n",
            "reward after 8219 epochs -20.500000 \n",
            "reward after 8224 epochs -21.000000 \n",
            "reward after 8234 epochs -21.000000 \n",
            "reward after 8261 epochs -21.000000 \n",
            "reward after 8267 epochs -20.000000 \n",
            "reward after 8277 epochs -21.000000 \n",
            "reward after 8285 epochs -21.000000 \n",
            "reward after 8286 epochs -20.000000 \n",
            "reward after 8290 epochs -21.000000 \n",
            "reward after 8329 epochs -20.000000 \n",
            "reward after 8334 epochs -21.000000 \n",
            "reward after 8358 epochs -20.000000 \n",
            "reward after 8363 epochs -20.000000 \n",
            "reward after 8367 epochs -20.000000 \n",
            "reward after 8370 epochs -21.000000 \n",
            "reward after 8390 epochs -21.000000 \n",
            "reward after 8408 epochs -21.000000 \n",
            "reward after 8415 epochs -20.000000 \n",
            "reward after 8424 epochs -20.000000 \n",
            "reward after 8426 epochs -20.000000 \n",
            "reward after 8427 epochs -21.000000 \n",
            "reward after 8430 epochs -20.000000 \n",
            "reward after 8435 epochs -20.000000 \n",
            "reward after 8451 epochs -20.000000 \n",
            "reward after 8468 epochs -20.000000 \n",
            "reward after 8473 epochs -20.000000 \n",
            "reward after 8506 epochs -20.000000 \n",
            "reward after 8507 epochs -20.500000 \n",
            "reward after 8546 epochs -21.000000 \n",
            "reward after 8547 epochs -20.000000 \n",
            "reward after 8560 epochs -21.000000 \n",
            "reward after 8568 epochs -20.000000 \n",
            "reward after 8582 epochs -21.000000 \n",
            "reward after 8584 epochs -20.000000 \n",
            "reward after 8590 epochs -20.000000 \n",
            "reward after 8596 epochs -20.000000 \n",
            "reward after 8628 epochs -20.000000 \n",
            "reward after 8644 epochs -21.000000 \n",
            "reward after 8657 epochs -21.000000 \n",
            "reward after 8660 epochs -20.000000 \n",
            "reward after 8661 epochs -20.000000 \n",
            "reward after 8676 epochs -21.000000 \n",
            "reward after 8684 epochs -21.000000 \n",
            "reward after 8697 epochs -21.000000 \n",
            "reward after 8721 epochs -20.000000 \n",
            "reward after 8723 epochs -20.000000 \n",
            "reward after 8735 epochs -20.000000 \n",
            "reward after 8739 epochs -20.000000 \n",
            "reward after 8747 epochs -20.000000 \n",
            "reward after 8754 epochs -21.000000 \n",
            "reward after 8759 epochs -21.000000 \n",
            "reward after 8788 epochs -21.000000 \n",
            "reward after 8797 epochs -20.000000 \n",
            "reward after 8801 epochs -20.000000 \n",
            "reward after 8805 epochs -21.000000 \n",
            "reward after 8826 epochs -21.000000 \n",
            "reward after 8833 epochs -20.000000 \n",
            "reward after 8844 epochs -21.000000 \n",
            "reward after 8862 epochs -20.000000 \n",
            "reward after 8878 epochs -20.000000 \n",
            "reward after 8886 epochs -20.000000 \n",
            "reward after 8898 epochs -21.000000 \n",
            "reward after 8906 epochs -21.000000 \n",
            "reward after 8927 epochs -20.000000 \n",
            "reward after 8933 epochs -21.000000 \n",
            "reward after 8934 epochs -20.000000 \n",
            "reward after 8942 epochs -21.000000 \n",
            "reward after 8962 epochs -21.000000 \n",
            "reward after 8972 epochs -21.000000 \n",
            "reward after 8973 epochs -21.000000 \n",
            "reward after 8993 epochs -21.000000 \n",
            "reward after 9013 epochs -20.000000 \n",
            "reward after 9035 epochs -20.000000 \n",
            "reward after 9037 epochs -20.000000 \n",
            "reward after 9043 epochs -21.000000 \n",
            "reward after 9056 epochs -21.000000 \n",
            "reward after 9065 epochs -21.000000 \n",
            "reward after 9072 epochs -20.000000 \n",
            "reward after 9084 epochs -21.000000 \n",
            "reward after 9095 epochs -20.000000 \n",
            "reward after 9103 epochs -21.000000 \n",
            "reward after 9111 epochs -20.000000 \n",
            "reward after 9137 epochs -21.000000 \n",
            "reward after 9151 epochs -20.000000 \n",
            "reward after 9153 epochs -20.000000 \n",
            "reward after 9167 epochs -21.000000 \n",
            "reward after 9186 epochs -20.000000 \n",
            "reward after 9187 epochs -21.000000 \n",
            "reward after 9208 epochs -21.000000 \n",
            "reward after 9221 epochs -21.000000 \n",
            "reward after 9232 epochs -21.000000 \n",
            "reward after 9233 epochs -21.000000 \n",
            "reward after 9234 epochs -20.000000 \n",
            "reward after 9244 epochs -21.000000 \n",
            "reward after 9245 epochs -21.000000 \n",
            "reward after 9267 epochs -21.000000 \n",
            "reward after 9281 epochs -21.000000 \n",
            "reward after 9290 epochs -20.000000 \n",
            "reward after 9296 epochs -20.000000 \n",
            "reward after 9306 epochs -20.000000 \n",
            "reward after 9327 epochs -21.000000 \n",
            "reward after 9334 epochs -21.000000 \n",
            "reward after 9344 epochs -21.000000 \n",
            "reward after 9368 epochs -21.000000 \n",
            "reward after 9373 epochs -20.000000 \n",
            "reward after 9381 epochs -21.000000 \n",
            "reward after 9395 epochs -21.000000 \n",
            "reward after 9406 epochs -20.000000 \n",
            "reward after 9420 epochs -20.000000 \n",
            "reward after 9426 epochs -21.000000 \n",
            "reward after 9447 epochs -20.000000 \n",
            "reward after 9456 epochs -20.000000 \n",
            "reward after 9457 epochs -21.000000 \n",
            "reward after 9474 epochs -20.000000 \n",
            "reward after 9483 epochs -20.000000 \n",
            "reward after 9506 epochs -20.000000 \n",
            "reward after 9512 epochs -20.000000 \n",
            "reward after 9517 epochs -21.000000 \n",
            "reward after 9522 epochs -21.000000 \n",
            "reward after 9545 epochs -20.000000 \n",
            "reward after 9547 epochs -21.000000 \n",
            "reward after 9556 epochs -21.000000 \n",
            "reward after 9565 epochs -20.000000 \n",
            "reward after 9595 epochs -20.000000 \n",
            "reward after 9596 epochs -20.000000 \n",
            "reward after 9600 epochs -21.000000 \n",
            "reward after 9622 epochs -20.000000 \n",
            "reward after 9627 epochs -21.000000 \n",
            "reward after 9637 epochs -21.000000 \n",
            "reward after 9638 epochs -21.000000 \n",
            "reward after 9646 epochs -20.000000 \n",
            "reward after 9661 epochs -20.000000 \n",
            "reward after 9682 epochs -21.000000 \n",
            "reward after 9686 epochs -20.000000 \n",
            "reward after 9692 epochs -21.000000 \n",
            "reward after 9702 epochs -21.000000 \n",
            "reward after 9721 epochs -21.000000 \n",
            "reward after 9722 epochs -21.000000 \n",
            "reward after 9732 epochs -21.000000 \n",
            "reward after 9769 epochs -21.000000 \n",
            "reward after 9785 epochs -20.500000 \n",
            "reward after 9791 epochs -21.000000 \n",
            "reward after 9794 epochs -21.000000 \n",
            "reward after 9796 epochs -20.000000 \n",
            "reward after 9806 epochs -20.000000 \n",
            "reward after 9822 epochs -21.000000 \n",
            "reward after 9828 epochs -21.000000 \n",
            "reward after 9832 epochs -20.000000 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-57f72e4b4c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocess_reshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-f916e003c38b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rank, preprocess_fn, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mobs_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mobs_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocess_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-b5c1bb82cc04>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(vec_env, obs, rews, dones, model, optimizer, last_actions, preprocess_fn, value_coeff, beta)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlast_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m#new_values = torch.cat((values,torch.zeros(1))).detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a674ba337c74>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L55HkqeKvdXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''def init_process(rank,size,fn,num_epochs,preprocess_fn,backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank,preprocess_fn,num_epochs)\n",
        "\n",
        "size = 4\n",
        "processes = []\n",
        "for rank in range(size):\n",
        "    p = Process(target=init_process, args=(rank, size, train, \\\n",
        "                                          num_epochs, preprocess_pong))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "    p.join()'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P13vsZGtT3FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
        "x[2] += 1\n",
        "print(x)\n",
        "x[0] = x[2]\n",
        "print(x)\n",
        "x[2] -= 1\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-VoLQXRWDy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # track rewards\n",
        "    dones = np.transpose(dones.numpy(),(1,0))\n",
        "    rews = np.transpose(rews.numpy(),(1,0))\n",
        "    for i in range(num_processes):\n",
        "      ind_done = np.argwhere(dones[i,:] == 1)\n",
        "      if ind_done.size == 0:\n",
        "        if i == 0:\n",
        "          rewards_list.append(rews[i,:])\n",
        "        running_reward_sum[i] += np.sum(rews[i,:])\n",
        "      else:\n",
        "        ind_done = ind_done[0][0] # not sure why a list of list\n",
        "        running_reward_sum[i] += np.sum(rews[i,:ind_done+1])\n",
        "        running_mean_reward[i] = running_reward_sum[i] if \\\n",
        "          running_mean_reward[i]==np.inf else running_mean_reward[i] * 0.99 + \\\n",
        "              running_reward_sum[i] * 0.01       \n",
        "        print(\"Process %d avg reward and reward after %d episodes: %f %f\"% \\\n",
        "              (i,epoch,running_mean_reward[i],running_reward_sum[i]))\n",
        "        running_reward_sum[i] = np.sum(rews[i,ind_done+1:])\n",
        "        \n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}