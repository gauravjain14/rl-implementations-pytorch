{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c-pytorchv2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrUvPRdjL6WbMXoSJQvck5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravjain14/rl-implementations-pytorch/blob/master/a2c_pytorchv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNDCKhEg3D5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.9.0\n",
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzXlK6w-3IKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines.common.cmd_util import *\n",
        "from stable_baselines.common.vec_env import VecFrameStack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_fSJQVpoPDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "\n",
        "import gym\n",
        "\n",
        "render = False\n",
        "update_size = 5\n",
        "num_processes = 16\n",
        "n_stack = 4\n",
        "env_id = 'PongNoFrameskip-v4'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj_OSLxnoX6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feed forward network as described in Asynchronous methods for deep reinforcement learning\n",
        "class ActorCriticFF(nn.Module):\n",
        "  def __init__(self,inp_channels,dimh,dimw,actor_dim,critic_dim):\n",
        "    super(ActorCriticFF,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(inp_channels,32,kernel_size=8,stride=4)\n",
        "    self.conv2 = nn.Conv2d(32,32,kernel_size=4,stride=2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    def conv2d_size_out(size,kernel_size,stride):\n",
        "      return (size - (kernel_size - 1) - 1) // stride + 1    \n",
        "\n",
        "    convw = conv2d_size_out(conv2d_size_out(dimw,8,4),4,2)\n",
        "    convh = conv2d_size_out(conv2d_size_out(dimh,8,4),4,2)\n",
        "\n",
        "    self.linear1 = nn.Linear(convh*convw*32, 256)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.actor = nn.Linear(256,actor_dim)\n",
        "    self.critic = nn.Linear(256,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.relu(self.conv1(x))\n",
        "    x = self.relu(self.conv2(x))\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.relu(self.linear1(x))\n",
        "\n",
        "    actor_out = self.softmax(self.actor(x))\n",
        "    critic_out = self.critic(x)\n",
        "    return actor_out,critic_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_Tmw_d7xM7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Is any kind of preprocessing required?\n",
        "def preprocess_pong(x):\n",
        "  resize = transforms.Compose([\n",
        "\t\ttransforms.ToPILImage(), # because pytorch tutorial does so\n",
        "\t\ttransforms.CenterCrop(80),\n",
        "\t\ttransforms.ToTensor()])\n",
        "  \n",
        "  return resize(x).unsqueeze(0)\n",
        "\n",
        "\"\"\" Gradient averaging. \"\"\"\n",
        "def average_gradients(model):\n",
        "    size = float(dist.get_world_size())\n",
        "    for param in model.parameters():\n",
        "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
        "        param.grad.data /= size\n",
        "\n",
        "def discount_rewards_per_proc(rewards,dones,last_value,gamma=0.99):\n",
        "  discounted_r = np.zeros_like(rewards) # should be 1-D\n",
        "  running_add = 0.\n",
        "  if dones[-1] == 0:\n",
        "    running_add = last_value\n",
        "  for t in reversed(range(0,len(rewards))):\n",
        "    running_add = rewards[t] + (1-dones[t])*gamma*running_add\n",
        "    discounted_r[t] = running_add\n",
        "\n",
        "  return discounted_r\n",
        "\n",
        "def discount_rewards_batch(rewards,dones,last_values,gamma=0.99):\n",
        "  # assume rewards shape - num_proc x update_length\n",
        "  batch_size = rewards.shape[0]\n",
        "  discounted_r = np.zeros_like(rewards)\n",
        "  for i in range(batch_size):\n",
        "    discounted_r[i] = discount_rewards_per_proc(rewards[i,:],dones[i,:], \\\n",
        "                                                last_values[i])\n",
        "  return discounted_r\n",
        "\n",
        "def discount_rewards(rewards,dones,last_value,gamma=0.99):\n",
        "  discounted_r = np.zeros_like(rewards)\n",
        "  #running_add = (1-dones[-1,:])*last_value\n",
        "  running_add = np.zeros((1,rewards.shape[1]))\n",
        "  for t in reversed(range(0,update_size)):\n",
        "    running_add = rewards[t,:] + (1-dones[t,:])*(gamma*running_add)\n",
        "    discounted_r[t] = running_add\n",
        "\n",
        "  return discounted_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZxKwiuml3Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Storage():\n",
        "  def __init__(self,obs_dim,num_processes=1,max_depth=5,dtype=np.float32):\n",
        "    self.obs_buf = np.ndarray((max_depth+1,num_processes,*obs_dim),dtype=dtype)\n",
        "    self.rew_buf = np.ndarray((max_depth,num_processes,),dtype=dtype)\n",
        "    self.logp_buf = np.ndarray((max_depth,num_processes,),dtype=dtype)\n",
        "    self.values_buf = np.ndarray((max_depth,num_processes,),dtype=dtype)\n",
        "    self.done_buf = np.ndarray((max_depth,num_processes,),dtype=np.int)\n",
        "    self.last_actions = None\n",
        "    self.curr_idx,self.max_depth = 0,max_depth\n",
        "\n",
        "  def store(self,obs,rew,logp,done,value):\n",
        "    assert(self.curr_idx < self.max_depth)\n",
        "    self.obs_buf[self.curr_idx+1] = obs\n",
        "    self.rew_buf[self.curr_idx] = rew\n",
        "    self.logp_buf[self.curr_idx] = logp\n",
        "    self.done_buf[self.curr_idx] = done\n",
        "    self.values_buf[self.curr_idx] = value\n",
        "    self.curr_idx += 1\n",
        "\n",
        "  def store_last_actions(self,last_actions):\n",
        "    self.last_actions = last_actions\n",
        "\n",
        "  def get(self):\n",
        "    # create tensors\n",
        "    data = {}\n",
        "    data['obs'] = self.obs_buf[1:,].swapaxes(0,1)\n",
        "    data['rew'] = self.rew_buf.swapaxes(0,1)\n",
        "    data['logp'] = self.logp_buf.swapaxes(0,1)\n",
        "    data['done'] = self.done_buf.swapaxes(0,1)\n",
        "    data['value'] = self.values_buf.swapaxes(0,1)\n",
        "\n",
        "    return {k: torch.tensor(v) for k,v in data.items()}\n",
        "\n",
        "  def get_last_actions(self):\n",
        "    return self.last_actions\n",
        "\n",
        "  # store only obs\n",
        "  def store_obs(self,obs,pos=0):\n",
        "    self.obs_buf[pos] = obs\n",
        "\n",
        "  def get_obs(self,pos=0):\n",
        "    return torch.tensor(self.obs_buf[pos])\n",
        "\n",
        "  def rollover(self):\n",
        "    self.obs_buf[0,:] = self.obs_buf[-1,:]    \n",
        "    self.curr_idx = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ATXjwDoqAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trajectory(env,model,preprocess_fn,storage,update_size=5):\n",
        "  reward_infos = []\n",
        "\n",
        "  x = storage.get_obs(0)\n",
        "  for i in range(update_size):\n",
        "    with torch.no_grad():\n",
        "      action_probs,critic_value = model(x)\n",
        "      m = Categorical(action_probs)\n",
        "      action = m.sample()\n",
        "\n",
        "    next_x,rew,done,infos = env.step(action)\n",
        "    for info in infos:\n",
        "      if 'episode' in info.keys():\n",
        "        reward_infos.append(info['episode']['r'])\n",
        "\n",
        "    storage.store(preprocess_fn(next_x), rew, -m.log_prob(action), \\\n",
        "                  done, critic_value.squeeze())\n",
        "    x = storage.get_obs(i)\n",
        "\n",
        "  # store last actions\n",
        "  storage.store_last_actions(action)    \n",
        "  return reward_infos\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo33QuOvolQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update(vec_env,obs,rews,dones,model,optimizer,last_actions, \\\n",
        "            preprocess_fn,value_coeff=0.5,beta=0.01):\n",
        "  action_probs,values = model(obs)\n",
        "  values = values.squeeze()\n",
        "  m = Categorical(action_probs)\n",
        "  actions = m.sample()\n",
        "  logProbs = -m.log_prob(actions)\n",
        "  entropy = m.entropy()\n",
        "\n",
        "  # what my understanding from A3C/A2C is \n",
        "  # perform an action to obtain the next value - for non terminal state\n",
        "  last_obs,last_rew,_,_ = vec_env.step(last_actions)\n",
        "  last_obs = torch.tensor(preprocess_fn(last_obs),dtype=torch.float32)\n",
        "  with torch.no_grad():\n",
        "    _,last_value = model(last_obs)\n",
        "\n",
        "  #new_values = torch.cat((values,torch.zeros(1))).detach()\n",
        "  last_value = last_value.numpy()\n",
        "  dones = dones.numpy()\n",
        "  returns = torch.as_tensor(discount_rewards_batch(rews.numpy(),dones, \\\n",
        "                                  last_value))\n",
        "  returns = returns.view(-1).detach()\n",
        "  advantage = returns - values\n",
        "\n",
        "  # we need to detach advantage, right?\n",
        "  actor_loss = torch.mean(-logProbs*advantage.detach())\n",
        "  mseLoss = nn.MSELoss()\n",
        "  critic_loss = mseLoss(returns,values)\n",
        "\n",
        "  total_loss = actor_loss + value_coeff*critic_loss #- beta*entropy.mean() \n",
        "  total_loss.backward()\n",
        "\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Han3GDYU-T12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_reshape(inp):\n",
        "  # expects numpy array\n",
        "  return inp.transpose((0,3,1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZJRDiFv-i3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(rank,preprocess_fn, num_epochs):\n",
        "  # vec_env resets after done. So we need to do this only once\n",
        "  # x is now - num_processes, height, width, num_channels\n",
        "  env = make_atari_env(env_id=env_id,num_env=num_processes,seed=23456)\n",
        "  vec_env = VecFrameStack(env,n_stack=n_stack)  \n",
        "  x = preprocess_fn(vec_env.reset())\n",
        "  n_acts = vec_env.action_space.n\n",
        "\n",
        "  storage = Storage(x.shape[1:],num_processes)\n",
        "  ac = ActorCriticFF(x.shape[1],x.shape[2],x.shape[3],n_acts,1)\n",
        "  optimizer = optim.Adam(ac.parameters(),lr=1e-3)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  storage.store_obs(x)\n",
        "  running_reward_sum = np.zeros((num_processes,1),dtype=np.float32)\n",
        "  running_mean_reward = np.empty((num_processes,1),dtype=np.float32)\n",
        "  running_mean_values = np.empty((num_processes,1),dtype=np.float32)\n",
        "  running_mean_reward.fill(np.inf)\n",
        "  running_mean_values.fill(np.inf)\n",
        "\n",
        "  reward_infos = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    reward_info = trajectory(vec_env,ac,preprocess_fn,storage)\n",
        "    if len(reward_info) > 0:\n",
        "      print(\"reward after %d epochs %f \"% \\\n",
        "            (epoch,sum(reward_info)/len(reward_info)))\n",
        "\n",
        "    data = storage.get()\n",
        "    obs,rews,logp,dones,values = data['obs'], data['rew'], \\\n",
        "                data['logp'],data['done'],data['value']\n",
        "\n",
        "    obs_shape = obs.shape[2:]\n",
        "    update(vec_env,obs.view(-1,*obs_shape),rews,dones,ac,optimizer, \\\n",
        "           storage.get_last_actions(),preprocess_fn)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    storage.rollover()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l485EAThBFwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(0,preprocess_reshape,100000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}